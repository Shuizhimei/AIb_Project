{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算一个图片的透视畸变的强度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "透视扭曲程度: 71.0\n"
     ]
    }
   ],
   "source": [
    "# 计算一个图片的透视畸变的强度\n",
    "def image_perspective_distortion(image_path):\n",
    "    # 读取图像\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # 将图像转换为灰度\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 使用Canny边缘检测\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "    # 使用霍夫变换检测直线\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, threshold=100)\n",
    "    \n",
    "    if lines is None:\n",
    "        return 0\n",
    "    # 计算直线的斜率和截距\n",
    "    slopes = []\n",
    "    intercepts = []\n",
    "    for line in lines:\n",
    "        rho, theta = line[0]\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        x1 = int(x0 + 1000 * (-b))\n",
    "        y1 = int(y0 + 1000 * (a))\n",
    "        x2 = int(x0 - 1000 * (-b))\n",
    "        y2 = int(y0 - 1000 * (a))\n",
    "\n",
    "        # 避免竖直线导致的除零错误\n",
    "        if x1 != x2:\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "            intercept = y1 - slope * x1\n",
    "            slopes.append(slope)\n",
    "            intercepts.append(intercept)\n",
    "\n",
    "    # 计算消失点的平均位置\n",
    "    if slopes:\n",
    "        avg_slope = np.mean(slopes)\n",
    "        avg_intercept = np.mean(intercepts)\n",
    "        vanishing_point_x = int(image.shape[1] / 2)\n",
    "        vanishing_point_y = int(avg_slope * vanishing_point_x + avg_intercept)\n",
    "\n",
    "        # 计算透视畸变的强度（消失点到图像中心的距离）\n",
    "        distortion_strength = np.linalg.norm([vanishing_point_x - image.shape[1] / 2, vanishing_point_y - image.shape[0] / 2])\n",
    "\n",
    "        return distortion_strength\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 使用示例\n",
    "image_path = 'data/mini-imagenet/images/dugong/n0207436700000408.jpg'\n",
    "print(\"透视扭曲程度: \"+str(image_perspective_distortion(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算数据集的valueList（并保存）， 加载给定路径的valueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valueList(path):\n",
    "    path_class = os.listdir(path)\n",
    "    valueList = []\n",
    "    for i in tqdm(path_class):\n",
    "        class_i = os.path.join(path, i)\n",
    "        \n",
    "        # 遍历类别下的每张图片\n",
    "        distortion_strength = 0\n",
    "        for image_name in os.listdir(class_i):\n",
    "            image_path = os.path.join(class_i, image_name)\n",
    "            distortion_strength += image_perspective_distortion(image_path)\n",
    "            \n",
    "        \n",
    "        mean_distortion_strength = distortion_strength / len(os.listdir(class_i))    \n",
    "        valueList.append(mean_distortion_strength)   \n",
    "    \n",
    "    # 创建一个文件夹叫valueList，如果它不存在的话\n",
    "    valueList_folder = 'valueList_for_perpectiveDistortion'\n",
    "    os.makedirs(valueList_folder, exist_ok=True)\n",
    "    \n",
    "    # 将结果保存\n",
    "    start_index = path.find('data/') + len('data/')\n",
    "    end_index = path.find('/', start_index)\n",
    "    result_string = path[start_index:end_index]  \n",
    "    valueList_file_path = os.path.join(valueList_folder, f'{result_string}_valueList.pkl')\n",
    "    with open(valueList_file_path, 'wb') as f:\n",
    "        pickle.dump(valueList, f)\n",
    "    \n",
    "    return valueList\n",
    "\n",
    "\n",
    "def load_valueList(path):\n",
    "    # 加载 valueList\n",
    "    valueList_folder = 'valueList_for_perpectiveDistortion'\n",
    "    start_index = path.find('data/') + len('data/')\n",
    "    end_index = path.find('/', start_index)\n",
    "    result_string = path[start_index:end_index]  \n",
    "    valueList_file_path = os.path.join(valueList_folder, f'{result_string}_valueList.pkl')\n",
    "    \n",
    "    with open(valueList_file_path, 'rb') as f:\n",
    "        loaded_valueList = pickle.load(f)    \n",
    "    return loaded_valueList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定两个数据集的valueList, 计算两个数据集之间的 perspective_distortion 关系 (由heuristic决定计算方式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个数据集之间的 perspective_distortion 关系\n",
    "def dataset_perspective_distortion(source_valueList,target_valueList,alpha,heuristic):\n",
    "     # 创建 权重矩阵\n",
    "    weight_matrix = np.ones((len(source_valueList),len(target_valueList)))  \n",
    "    weight_matrix = weight_matrix/np.sum(weight_matrix) \n",
    "    \n",
    "    return heuristic(source_valueList,target_valueList,weight_matrix,alpha = alpha)\n",
    "    \n",
    "# 越大说明比源域的 透视畸变 更大\n",
    "def fun1(list1,list2,weight_matrix,alpha=0.01):\n",
    "    num = 0\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list2)):\n",
    "            \n",
    "            num += weight_matrix[i][j] * (list1[i]-list2[j])\n",
    "\n",
    "    # return math.e ** (-1*alpha*tmp)    \n",
    "    return num/(len(list1)*len(list2))\n",
    "\n",
    "# 越大说明比源域的 透视畸变 更小\n",
    "def fun2(list1,list2,weight_matrix,alpha=0.01):\n",
    "    num = 0\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list2)):\n",
    "            \n",
    "            num += weight_matrix[i][j] * (list1[i]-list2[j])\n",
    "\n",
    "    return math.e ** (-1*alpha*num)    \n",
    "\n",
    "## 越大说明与源域的 透视畸变 越相似\n",
    "def fun3(list1,list2,weight_matrix,alpha=0.01):\n",
    "    num = 0\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list2)):\n",
    "            \n",
    "            num += weight_matrix[i][j] * abs(list1[i]-list2[j])\n",
    "\n",
    "    return math.e ** (-1*alpha*num)   \n",
    "\n",
    "\n",
    "## 不考虑 list1（即源域）， 越大说明透视畸变 越小\n",
    "def fun4(list1,list2,weight_matrix,alpha=0.01):\n",
    "    num = 0\n",
    "    \n",
    "    weight_matrix = np.ones((1,len(list2)))\n",
    "    weight_matrix = weight_matrix/np.sum(weight_matrix)\n",
    "    \n",
    "    for j in range(len(list2)):\n",
    "        \n",
    "        num += weight_matrix[0][j] * list2[j]\n",
    "\n",
    "    return math.e ** (-1*alpha*num)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUB\n",
    "CUB_path = 'data/CUB_200_2011/images/'\n",
    "\n",
    "# Stanford Cars\n",
    "Cars_path = 'data/stanford_cars/images/'\n",
    "\n",
    "# EuroSAT\n",
    "EuroSAT_path = 'data/EuroSAT_RGB/'\n",
    "\n",
    "# Plant disease\n",
    "Plant_path = 'data/Plant_disease/images'\n",
    "\n",
    "# mini-imagenet\n",
    "mini_path = 'data/mini-imagenet/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:31:02<00:00, 54.63s/it]\n",
      "100%|██████████| 200/200 [10:26<00:00,  3.13s/it]\n",
      "100%|██████████| 196/196 [32:55<00:00, 10.08s/it]\n",
      "100%|██████████| 10/10 [00:36<00:00,  3.64s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# mini_valueList = get_valueList(mini_path)\n",
    "\n",
    "# CUB_valueList = get_valueList(CUB_path)\n",
    "# Cars_valueList = get_valueList(Cars_path)\n",
    "# EuroSAT_valueList = get_valueList(EuroSAT_path)\n",
    "# Plant_valueList = get_valueList(Plant_path)\n",
    "\n",
    "mini_valueList = load_valueList(mini_path)\n",
    "CUB_valueList = load_valueList(CUB_path)\n",
    "Cars_valueList = load_valueList(Cars_path)\n",
    "EuroSAT_valueList = load_valueList(EuroSAT_path)\n",
    "Plant_valueList = load_valueList(Plant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013065639645103288\n",
      "0.002145576998879036\n",
      "0.09880474463064076\n",
      "0.05879522719673957\n"
     ]
    }
   ],
   "source": [
    "print(dataset_perspective_distortion(mini_valueList,CUB_valueList,0.01,fun1))\n",
    "print(dataset_perspective_distortion(mini_valueList,Cars_valueList,0.01,fun1))\n",
    "print(dataset_perspective_distortion(mini_valueList,EuroSAT_valueList,0.01,fun1))\n",
    "print(dataset_perspective_distortion(mini_valueList,Plant_valueList,0.01,fun1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7700400188888097\n",
      "0.6566966469015908\n",
      "0.3723029232355211\n",
      "0.6247761723834998\n"
     ]
    }
   ],
   "source": [
    "print(dataset_perspective_distortion(mini_valueList,CUB_valueList,0.01,fun2))\n",
    "print(dataset_perspective_distortion(mini_valueList,Cars_valueList,0.01,fun2))\n",
    "print(dataset_perspective_distortion(mini_valueList,EuroSAT_valueList,0.01,fun2))\n",
    "print(dataset_perspective_distortion(mini_valueList,Plant_valueList,0.01,fun2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5892485969376481\n",
      "0.5990748203065243\n",
      "0.3723029232355211\n",
      "0.539139459396691\n"
     ]
    }
   ],
   "source": [
    "print(dataset_perspective_distortion(mini_valueList,CUB_valueList,0.01,fun3))\n",
    "print(dataset_perspective_distortion(mini_valueList,Cars_valueList,0.01,fun3))\n",
    "print(dataset_perspective_distortion(mini_valueList,EuroSAT_valueList,0.01,fun3))\n",
    "print(dataset_perspective_distortion(mini_valueList,Plant_valueList,0.01,fun3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.483484581761772\n",
      "0.5669322026065907\n",
      "0.99999880000072\n",
      "0.5958973676156077\n"
     ]
    }
   ],
   "source": [
    "print(dataset_perspective_distortion(mini_valueList,CUB_valueList,0.01,fun4))\n",
    "print(dataset_perspective_distortion(mini_valueList,Cars_valueList,0.01,fun4))\n",
    "print(dataset_perspective_distortion(mini_valueList,EuroSAT_valueList,0.01,fun4))\n",
    "print(dataset_perspective_distortion(mini_valueList,Plant_valueList,0.01,fun4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
