{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this file, you need to load datasets from `sharedata/data_for_compute_similarity/data` into your `data` folder, where there are 5 datasets.\n",
    "\n",
    "that is, you may use the command `cp -r /shareddata/data_for_compute_similarity/data/ .`\n",
    "\n",
    "Additionally, you also need to load clip model by the command `cp -r /data/lab/STA303-Assignment02/clip/ .`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deal with the mini-imagenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义数据集目录\n",
    "# dataset_dir = \"data/mini-imagenet/images/\"\n",
    "\n",
    "# os.remove(\"data/mini-imagenet/images/.DS_Store\")\n",
    "# # 遍历数据集目录下的所有图片文件\n",
    "# for filename in os.listdir(dataset_dir):\n",
    "#     if filename.endswith(\".jpg\"):\n",
    "#         # 除去文件名的后四位数字\n",
    "#         class_id = filename[0:9]\n",
    "        \n",
    "#         # 构建目标类别目录\n",
    "#         target_class_dir = os.path.join(dataset_dir, class_id)\n",
    "#         os.makedirs(target_class_dir, exist_ok=True)\n",
    "        \n",
    "#         # 构建源文件路径和目标文件路径\n",
    "#         source_path = os.path.join(dataset_dir, filename)\n",
    "#         target_path = os.path.join(target_class_dir, filename)\n",
    "        \n",
    "#         # 移动文件到目标类别目录\n",
    "#         shutil.move(source_path, target_path)\n",
    "\n",
    "# print(\"操作完成。\")\n",
    "# tmp = os.listdir(dataset_dir)\n",
    "# print(len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge the training set and testing set (`Plant disease` and `standford cars`) to the directory `images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_dataset(dataset_dir):\n",
    "#     # 定义训练集和测试集目录\n",
    "#     train_dir = os.path.join(dataset_dir, \"train\")\n",
    "#     test_dir = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "#     # 获取训练集和测试集中的类别\n",
    "#     train_classes = set(os.listdir(train_dir))\n",
    "#     test_classes = set(os.listdir(test_dir))\n",
    "\n",
    "#     # 检查训练集和测试集中的类别是否一致\n",
    "#     if train_classes != test_classes:\n",
    "#         raise ValueError(\"训练集和测试集中的类别不一致，请检查数据集。\")\n",
    "\n",
    "#     for subset_dir in [train_dir, test_dir]:\n",
    "#         for class_name in os.listdir(subset_dir):\n",
    "#             class_dir = os.path.join(subset_dir, class_name)\n",
    "            \n",
    "#             # 遍历类别目录下的所有图片文件\n",
    "#             for filename in os.listdir(class_dir):\n",
    "#                 if filename.endswith(\".jpg\"):\n",
    "#                     # 构建目标类别目录\n",
    "#                     target_class_dir = os.path.join(dataset_dir, \"images\", class_name)\n",
    "#                     os.makedirs(target_class_dir, exist_ok=True)\n",
    "                    \n",
    "#                     # 构建源文件路径和目标文件路径\n",
    "#                     source_path = os.path.join(class_dir, filename)\n",
    "#                     target_path = os.path.join(target_class_dir, filename)\n",
    "                    \n",
    "#                     # 移动文件到目标类别目录\n",
    "#                     shutil.move(source_path, target_path)\n",
    "\n",
    "\n",
    "#     shutil.rmtree(train_dir)\n",
    "#     shutil.rmtree(test_dir)\n",
    "\n",
    "#     print(\"操作完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_dataset('data/Plant_disease/')\n",
    "# merge_dataset('data/stanford_cars/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from clip import clip\n",
    "import pickle\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01 \n",
    "VISUAL_BACKBONE = 'ViT-B/32' # RN50, ViT-B/32, ViT-B/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, transform = clip.load(VISUAL_BACKBONE, device=device)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(transform,image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## try one image \n",
    "# image_path = \"data/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg\"\n",
    "\n",
    "# image_tensor = preprocess_image(transform,image_path)\n",
    "# vec_features = model.encode_image(image_tensor)\n",
    "# print(vec_features)\n",
    "# print(vec_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Sim(D_B,D_N) = exp(-\\alpha ~EMD(D_B,D_N))$\n",
    "\n",
    "where \n",
    "$EMD(D_B,D_N) = \\frac{\\sum_{i \\in C_B, ~j \\in C_N}{f_{i,j}~d_{i,j}}}{\\sum_{i \\in C_B, ~j \\in C_N}{f_{i,j}}},~~\\alpha ~\\text{is typically set to} ~ 0.01$\n",
    "\n",
    "where $d_{i,j} = \\Vert p_i-p_j \\Vert_2 $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUB\n",
    "CUB_path = 'data/CUB_200_2011/images/'\n",
    "\n",
    "# Stanford Cars\n",
    "Cars_path = 'data/stanford_cars/images/'\n",
    "\n",
    "# EuroSAT\n",
    "EuroSAT_path = 'data/EuroSAT_RGB/'\n",
    "\n",
    "# Plant disease\n",
    "Plant_path = 'data/Plant_disease/images'\n",
    "\n",
    "# mini-imagenet\n",
    "mini_path = 'data/mini-imagenet/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算源域的向量 (每调一次超参跑一次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_sourceVector(model,transform):\n",
    "    # 分别得到两个数据集的标签集\n",
    "    mini_class = os.listdir(mini_path)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # 遍历第一个数据集的标签\n",
    "        mini_vectorlist = [] \n",
    "        for i in tqdm(mini_class):   \n",
    "            ##* 对于 dataset1 的 i类 图像， 提取出平均特征 p_i\n",
    "            class_i = os.path.join(mini_path, i)\n",
    "            \n",
    "            p_i = 0\n",
    "            \n",
    "            # 遍历类别下的每张图片\n",
    "            for image_name in os.listdir(class_i):\n",
    "                image_path = os.path.join(class_i, image_name)\n",
    "                image_tensor = preprocess_image(transform,image_path)\n",
    "                image_tensor = image_tensor.to(device)\n",
    "                p_i += model.encode_image(image_tensor)\n",
    "            \n",
    "            p_i = p_i / len(os.listdir(class_i)) \n",
    "\n",
    "            mini_vectorlist.append(p_i)\n",
    "\n",
    "\n",
    "    # 创建一个文件夹叫vectorlist，如果它不存在的话\n",
    "    vectorlist_folder = 'vectorlist_for_similarity'\n",
    "    os.makedirs(vectorlist_folder, exist_ok=True)\n",
    "\n",
    "    # 保存 mini_vectorlist\n",
    "    vectorlist_file_path = os.path.join(vectorlist_folder, 'mini_vectorlist.pkl')\n",
    "    with open(vectorlist_file_path, 'wb') as f:\n",
    "        pickle.dump(mini_vectorlist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源域与数据集之间的similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#源域与数据集之间的similarity\n",
    "def similarity_withSource(model,transform,target_path,alpha,source_vectorlist):\n",
    "    # 分别得到 target 数据集的标签列表\n",
    "    target_class = os.listdir(target_path)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # 遍历第二个数据集的标签\n",
    "        target_vectorlist = []\n",
    "        for j in tqdm(target_class):\n",
    "            ##* 对于 target_path 的 第j类 图像，提取出平均特征 p_j\n",
    "            class_j = os.path.join(target_path, j)\n",
    "            \n",
    "            p_j = 0\n",
    "            # 遍历类别下的每张图片\n",
    "            for image_name in os.listdir(class_j):\n",
    "                image_path = os.path.join(class_j, image_name)\n",
    "                image_tensor = preprocess_image(transform,image_path)\n",
    "                image_tensor = image_tensor.to(device)\n",
    "                p_j += model.encode_image(image_tensor)\n",
    "            \n",
    "            p_j = p_j / len(os.listdir(class_j))    \n",
    "            target_vectorlist.append(p_j)        \n",
    "            \n",
    "        start_index = target_path.find('data/') + len('data/')\n",
    "        end_index = target_path.find('/', start_index)\n",
    "        result_string = target_path[start_index:(end_index-1)]  \n",
    "        vectorlist_file_path = os.path.join(\"vectorlist_for_similarity\", f'{result_string}_vectorlist.pkl')\n",
    "        with open(vectorlist_file_path, 'wb') as f:\n",
    "            pickle.dump(target_vectorlist, f)\n",
    "    \n",
    "        # 创建 权重矩阵\n",
    "        weight_matrix = np.ones((len(source_vectorlist),len(target_vectorlist)))  \n",
    "        weight_matrix = weight_matrix/np.sum(weight_matrix)\n",
    "         \n",
    "        # 计算\n",
    "        EMD = 0\n",
    "        for i in range(len(source_vectorlist)):\n",
    "            for j in range(len(target_vectorlist)):\n",
    "                \n",
    "                ##* 将两个求二范数  d_ij\n",
    "                EMD += weight_matrix[i][j] * torch.norm(source_vectorlist[i] - target_vectorlist[j], p=2)\n",
    "\n",
    "    return math.e ** (-1*alpha*EMD)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意两个数据集之间的similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意两个数据集之间的similarity\n",
    "\n",
    "# def similarity(dataset1,dataset2,alpha):\n",
    "#     # 分别得到两个数据集的标签列表\n",
    "#     label_class1 = os.listdir(dataset1)\n",
    "#     label_class2 = os.listdir(dataset2)\n",
    "        \n",
    "#     with torch.no_grad():\n",
    "#         # 遍历第一个数据集的标签\n",
    "#         dataset1_vectorlist = [] \n",
    "#         for i in tqdm(label_class1):   \n",
    "#             ##* 对于 dataset1 的 i类 图像， 提取出平均特征 p_i\n",
    "#             class_i = os.path.join(dataset1, i)\n",
    "            \n",
    "#             p_i = 0\n",
    "            \n",
    "#             # 遍历类别下的每张图片\n",
    "#             for image_name in os.listdir(class_i):\n",
    "#                 image_path = os.path.join(class_i, image_name)\n",
    "#                 image_tensor = preprocess_image(transform,image_path)\n",
    "#                 image_tensor = image_tensor.to(device)\n",
    "#                 p_i += model.encode_image(image_tensor)\n",
    "            \n",
    "#             p_i = p_i / len(os.listdir(class_i)) \n",
    "\n",
    "#             dataset1_vectorlist.append(p_i)\n",
    "            \n",
    "#         # 遍历第二个数据集的标签\n",
    "#         dataset2_vectorlist = []\n",
    "#         for j in tqdm(label_class2):\n",
    "#             ##* 对于 dataset2 的 第j类 图像，提取出平均特征 p_j\n",
    "#             class_j = os.path.join(dataset2, j)\n",
    "            \n",
    "#             p_j = 0\n",
    "#             # 遍历类别下的每张图片\n",
    "#             for image_name in os.listdir(class_j):\n",
    "#                 image_path = os.path.join(class_j, image_name)\n",
    "#                 image_tensor = preprocess_image(transform,image_path)\n",
    "#                 image_tensor = image_tensor.to(device)\n",
    "#                 p_j += model.encode_image(image_tensor)\n",
    "            \n",
    "#             p_j = p_j / len(os.listdir(class_j))    \n",
    "#             dataset2_vectorlist.append(p_j)        \n",
    "            \n",
    "            \n",
    "#         # 创建 权重矩阵\n",
    "#         weight_matrix = np.ones((len(label_class1),len(label_class2)))  \n",
    "#         weight_matrix = weight_matrix/np.sum(weight_matrix)\n",
    "         \n",
    "#         # 计算\n",
    "#         EMD = 0\n",
    "#         for i in range(len(dataset1_vectorlist)):\n",
    "#             for j in range(len(dataset2_vectorlist)):\n",
    "                \n",
    "#                 ##* 将两个求二范数  d_ij\n",
    "#                 EMD += weight_matrix[i][j] * torch.norm(dataset1_vectorlist[i] - dataset2_vectorlist[j], p=2)\n",
    "\n",
    "#     return math.e ** (-1*alpha*EMD)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUB\n",
    "# CUB_path = 'data/CUB_200_2011/images/'\n",
    "\n",
    "# # Stanford Cars\n",
    "# Cars_path = 'data/stanford_cars/images/'\n",
    "\n",
    "# # EuroSAT\n",
    "# EuroSAT_path = 'data/EuroSAT_RGB/'\n",
    "\n",
    "# # Plant disease\n",
    "# Plant_path = 'data/Plant_disease/images'\n",
    "\n",
    "# # mini-imagenet\n",
    "# mini_path = 'data/mini-imagenet/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the hierarchy: `data_dir`->`(class1, class2, ...)`->`(image1.jpg, image2.jpg)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:36<14:37,  9.14s/it]"
     ]
    }
   ],
   "source": [
    "alpha = 0.01  \n",
    "VISUAL_BACKBONE = 'RN50' # RN50, ViT-B/32, ViT-B/16\n",
    "model, transform = clip.load(VISUAL_BACKBONE, device=device)\n",
    "model.to(device);\n",
    "\n",
    "reset_sourceVector(model,transform)\n",
    "\n",
    "# 加载 mini_vectorlist\n",
    "vectorlist_file_path = 'vectorlist_for_similarity/mini_vectorlist.pkl'\n",
    "with open(vectorlist_file_path, 'rb') as f:\n",
    "    loaded_mini_vectorlist = pickle.load(f)\n",
    "\n",
    "    \n",
    "CUB = similarity_withSource(model, transform, CUB_path,alpha,loaded_mini_vectorlist)\n",
    "print(CUB)\n",
    "\n",
    "Cars = similarity_withSource(model, transform, Cars_path,alpha,loaded_mini_vectorlist)\n",
    "print(Cars)\n",
    "\n",
    "EuroSAT = similarity_withSource(model, transform, EuroSAT_path,alpha,loaded_mini_vectorlist)\n",
    "print(EuroSAT)\n",
    "\n",
    "Plant = similarity_withSource(model, transform, Plant_path,alpha,loaded_mini_vectorlist)\n",
    "print(Plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01  \n",
    "VISUAL_BACKBONE = 'ViT-B/16' # RN50, ViT-B/32, ViT-B/16\n",
    "model, transform = clip.load(VISUAL_BACKBONE, device=device)\n",
    "model.to(device);\n",
    "\n",
    "reset_sourceVector(model,transform)\n",
    "\n",
    "# 加载 mini_vectorlist\n",
    "vectorlist_file_path = 'vectorlist_for_similarity/mini_vectorlist.pkl'\n",
    "with open(vectorlist_file_path, 'rb') as f:\n",
    "    loaded_mini_vectorlist = pickle.load(f)\n",
    "\n",
    "    \n",
    "CUB = similarity_withSource(model, transform, CUB_path,alpha,loaded_mini_vectorlist)\n",
    "print(CUB)\n",
    "\n",
    "Cars = similarity_withSource(model, transform, Cars_path,alpha,loaded_mini_vectorlist)\n",
    "print(Cars)\n",
    "\n",
    "EuroSAT = similarity_withSource(model, transform, EuroSAT_path,alpha,loaded_mini_vectorlist)\n",
    "print(EuroSAT)\n",
    "\n",
    "Plant = similarity_withSource(model, transform, Plant_path,alpha,loaded_mini_vectorlist)\n",
    "print(Plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01  \n",
    "VISUAL_BACKBONE = 'ViT-B/32' # RN50, ViT-B/32, ViT-B/16\n",
    "model, transform = clip.load(VISUAL_BACKBONE, device=device)\n",
    "model.to(device);\n",
    "\n",
    "reset_sourceVector(model,transform)\n",
    "\n",
    "# 加载 mini_vectorlist\n",
    "vectorlist_file_path = 'vectorlist_for_similarity/mini_vectorlist.pkl'\n",
    "with open(vectorlist_file_path, 'rb') as f:\n",
    "    loaded_mini_vectorlist = pickle.load(f)\n",
    "\n",
    "    \n",
    "CUB = similarity_withSource(model, transform, CUB_path,alpha,loaded_mini_vectorlist)\n",
    "print(CUB)\n",
    "\n",
    "Cars = similarity_withSource(model, transform, Cars_path,alpha,loaded_mini_vectorlist)\n",
    "print(Cars)\n",
    "\n",
    "EuroSAT = similarity_withSource(model, transform, EuroSAT_path,alpha,loaded_mini_vectorlist)\n",
    "print(EuroSAT)\n",
    "\n",
    "Plant = similarity_withSource(model, transform, Plant_path,alpha,loaded_mini_vectorlist)\n",
    "print(Plant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
